[
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html",
    "href": "Oxford_102_Flower_Tiny_DDPM.html",
    "title": "(Settings)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision import datasets\nfrom torchvision.transforms import v2\nfrom torch.utils.data import ConcatDataset, DataLoader\nimport matplotlib.pyplot as plt\nprint(torch.backends.mps.is_available())\n\nTrue\n# Priorities: CUDA -&gt; MPS -&gt; CPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Device: CUDA (NVIDIA GPU)\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Device: MPS (Mac Silicon)\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Device: CPU\")\n\nDevice: MPS (Mac Silicon)"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#load-dataset-and-preprocess",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#load-dataset-and-preprocess",
    "title": "(Settings)",
    "section": "1. Load Dataset and Preprocess",
    "text": "1. Load Dataset and Preprocess\n\ntransform\nToTensor:\n1) 값이 존재하는 범위를 Z에서 R로\n2) 범위 줄이기 -&gt; gradient(미분계수) 폭발 방지\nNormalize: 채널 3개라서 3개씩\n! num_workers: ?? 메인 프로세스는 학습만, 별도의 프로세스가 다음 배치 미리 읽어옴\n! pin_memory: ?? 데이터를 RAM의 아무데나 넣는게 아니라 고정된 위치에 넣음\n\nIMG_SIZE = 64\nBATCH_SIZE = 64\n\n# define trasform\ntransform = v2.Compose([\n    v2.Resize(IMG_SIZE), #1. resize shorter edge into 64\n    v2.CenterCrop(IMG_SIZE), #2. crop at the center(rectangle image -&gt; square image)\n    v2.ToImage(), #3. (H, W, C) -&gt; (C, H, W)\n    v2.ToDtype(torch.float32, scale=True), #4. Z[0, 255] -&gt; R[0.0, 1.0]\n    v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #5. Normalize [0.0, 1.0] -&gt; [-1.0, 1.0]\n])\n\n# load data\ntrain_set = datasets.Flowers102(root='./data', split='train', download=True, transform=transform)\nval_set = datasets.Flowers102(root='./data', split='val', download=True, transform=transform)\ntest_set = datasets.Flowers102(root='./data', split='test', download=True, transform=transform)\n\n# concaterate into one train dataset\ndataset = ConcatDataset([train_set, val_set, test_set])\ndel train_set, val_set, test_set\n\n# build dataloader\ndataloader = DataLoader(\n    dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    drop_last=True,\n)\n\n\n# Check batch shape\ndataloader_iterator = iter(dataloader) # dataloader: iterable -&gt; dataloader_iterator: iterater\nsample_batch = next(dataloader_iterator)[0] # bring only features of a batch, not labels\nprint(f\"Shape of Each Batch: {sample_batch.shape}\")\n\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n  warnings.warn(warn_msg)\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n\n\nShape of Each Batch: torch.Size([64, 3, 64, 64])\n\n\n\n# Visualize 10 sample images\n## fetch a single batch for visualization\nimages = next(dataloader_iterator)[0]\n\n## initialize subplots\nfig, axes = plt.subplots(1, 10, figsize=(15, 3)) # fig: top-level container, # axes: numpy array of each grid\nplt.subplots_adjust(wspace=0.1)\n\n## plot each subplots\nfor i in range(10):\n  img = images[i]\n  img = (img * 0.5) + 0.5 # unnormalize to [0, 1] for visualization\n  img = img.permute(1, 2, 0) # (C, H, W) -&gt; (H, W, C)\n  axes[i].imshow(img.cpu().numpy()) # display image\n  axes[i].axis('off')\n\nplt.suptitle(\"10 Sample Images from Oxford 102 Flowers\", y=0.8)\nplt.show()"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#diffusion-schedule-forward-process",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#diffusion-schedule-forward-process",
    "title": "(Settings)",
    "section": "2. Diffusion Schedule (Forward Process)",
    "text": "2. Diffusion Schedule (Forward Process)\n일단 Linear만 하고 나중에 Cosine이랑 Offset Cosine 추가\nsig_rates^2 + nos_rates^2 = 1\n\ndef linear_diffusion_schedule(T=1000):\n  # beta rates\n  min_beta = 0.0001\n  max_beta = 0.02\n\n  # betas, alphas, alpha_bars\n  betas = torch.linspace(min_beta, max_beta, T, device=device)\n  alphas = 1 - betas\n  alpha_bars = torch.cumprod(alphas, dim=0)\n\n  # signal rates, noise rates\n  signal_rates = torch.sqrt(alpha_bars)\n  noise_rates = torch.sqrt(1 - alpha_bars)\n\n  return signal_rates, noise_rates # shape: torch.Size([1000])\n\n\n# visualize signal power & noise power\ntimesteps = torch.linspace(0, 1, 1000) # Visualization works in CPU\nsig_rates, nos_rates = linear_diffusion_schedule() # on MPS/GPU\nsig_rates = sig_rates.detach().cpu() # move to CPU\nnos_rates = nos_rates.detach().cpu()\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n## plot signal power\naxes[0].plot(timesteps, sig_rates**2, color=\"tab:blue\", label=r\"$\\bar{\\alpha_t}$\")\naxes[0].legend()\naxes[0].set_title(\"Signal Power\")\naxes[0].grid(True, alpha=0.2)\n\n## plot noise power\naxes[1].plot(timesteps, nos_rates**2, color=\"tab:red\", label=r\"$1-\\bar{\\alpha_t}$\")\naxes[1].legend()\naxes[1].set_title(\"Noise Power\")\naxes[1].grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#model",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#model",
    "title": "(Settings)",
    "section": "3. Model",
    "text": "3. Model\n\n1) Architecture\n\n\n\nArchitecture\n\n\n\n\n2) Residual Block\n\n\n\nResBlock.jpg\n\n\n\ni) Sinusoidal Time Embedding\n\n\n\nSinusoidal_Time_Embedding.jpg\n\n\n\nclass TimeEmbeddingBlock(nn.Module):\n  def __init__(self, first_dim=128, global_dim=512):\n    super().__init__()\n    '''self.dims = torch.arange(first_dim // 2) -&gt; saved on CPU!''' #dims: [0, 1, 2, 3, ..., {(first_dim//2 - 1)=63}]\n    self.register_buffer('dims', torch.arange(first_dim // 2)) # auto move to GPU\n\n    # set mlp phase(128 -&gt; Linear Layer -&gt; 512 -&gt; SiLU -&gt; 512 -&gt; Linear Layer -&gt; 512)\n    self.mlp = nn.Sequential(\n        nn.Linear(first_dim, global_dim),\n        nn.SiLU(),\n        nn.Linear(global_dim, global_dim)\n    )\n\n  # First time embeddig (BATCH_SIZE, first_dim=128)\n  def generate_first_time_embeddings(self, times):\n    B = times.shape[0] # B = BATCH_SIZE\n\n    # denominators & frequencies\n    denominators = 1/(10000**(self.dims/self.dims[-1])) # exponent covers whole range [0, 1]\n    frequencies = times.reshape(B, -1)@denominators.reshape(1, -1) # (BATCH_SIZE, (first_dim//2) = 64)\n\n    # make each half(cos, sin) and concatenate\n    odd_half = torch.cos(frequencies) # left half\n    even_half = torch.sin(frequencies) # right half\n    first_time_embeddings = torch.cat((odd_half, even_half), dim=1) #(BATCH_SIZE, first_dim=128)\n\n    return first_time_embeddings # returns (B, fist_dim=128)\n\n  # Global time embeddings (BATCH_SIZE, global_dim=512)\n  def forward(self, times):\n    x = self.generate_first_time_embeddings(times) # generates first time embeddings (BATCH_SIZE, first_dim=128)\n    global_time_embeddings = self.mlp(x) # generates global time embeddings (BATCH_size, global_dim=512)\n\n    return global_time_embeddings # returns (B, global_dim=512). It must be reshaped to (B, C)\n\n\n\nii) Residual Block\n\nclass ResBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, dim_t=512): # fixed num_groups 32\n    super().__init__()\n\n    self.proj1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n    self.proj2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)\n\n    # If in/out channels are different -&gt; we need 1*1 Conv / else -&gt; Identitiy\n    if in_channels != out_channels:\n      self.proj_add = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n    else:\n      self.proj_add = nn.Identity()\n\n    self.proj_t = nn.Sequential(\n        nn.SiLU(),                      # i) activation(SiLU)\n        nn.Linear(dim_t, out_channels), # ii) reshape by a Linear Layer\n    )\n\n    self.groupnorm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n    self.groupnorm2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n\n    self.dropout = nn.Dropout(p=0.1)\n\n  def forward(self, x, global_time_emb):\n    # prepare for residual connection\n    x_add = x # save x\n    x_add = self.proj_add(x_add) # reshape if needed\n\n    # Before Adding Time Embedding\n    x = self.groupnorm1(x) # Group Norm 1\n    x = F.silu(x) # Swish\n    x = self.proj1(x) # 3*3 Conv 1\n\n    # Reshape and Add Time Embedding\n    t = self.proj_t(global_time_emb) # (BATCH, dim_t=512) -&gt; (BATCH, out_channels)\n    t = t[:, :, None, None] # (BATCH, out_channels) -&gt; (BATCH, out_channels, 1, 1)\n    x = x + t\n\n    # After Adding Time Embedding\n    x = self.groupnorm2(x) # Group Norm 2\n    x = F.silu(x) # Swish\n    x = self.dropout(x) # Droupout\n    x = self.proj2(x) # 3*3 Conv 2\n    x = x + x_add # residual connection\n\n    return x\n\n\n\n\n3) DownSample / UpSample\n\n\n\nDownSample.jpg\n\n\n\n# DownSample\nclass DownSample(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.downsample = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=1, stride=2)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        return x\n    \n# UpSample\nclass UpSample(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') # just upscale 1 pixel into 4 pixels\n        self.proj = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding = 1) # smooth out\n\n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.proj(x)\n        return x\n\n\n\n4) Attention Block\n\n# AttentionBlock\nclass AttentionBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n        # group normalization\n        self.groupnorm = nn.GroupNorm(num_groups=32, num_channels=dim)\n\n        # attention parameters (W_Q, W_K, W_V)\n        self.w_q = nn.Linear(dim, dim)  # w_q: (C, C)\n        self.w_k = nn.Linear(dim, dim)  # w_k: (C, C)\n        self.w_v = nn.Linear(dim, dim)  # w_v: (C, C)\n\n        # 1*1 convolution\n        self.proj = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=1)\n\n    def forward(self, x):\n        x_add = x # for residual connection\n        B, C, H, W = x.shape\n\n        # GroupNorm before attention\n        x = self.groupnorm(x)\n\n        # reshape for attention: (B, C, H, W) -&gt; (B, H * W, C)\n        x = x.reshape(B, C, -1) # (B, C, H, W) -&gt; (B, C, H * W)\n        x = x.permute(0, 2, 1) # (B, C, H * W) -&gt; (B, H * W, C)\n\n        # Query, Key, Value\n        q = self.w_q(x) # x @ w_q(k, v): (H * W, C) @ (C, C) -&gt; (H * W, C)\n        k = self.w_k(x)\n        v = self.w_v(x)\n\n        # Attention\n        inner_q_k = q @ k.transpose(-1, -2) # (H * W, C) @ (C, H * W) -&gt; (H * W, H * W)\n        attention_x = (F.softmax((inner_q_k/C**0.5), dim=-1)) @ v # (H * W, H * W) @ (H * W, C) -&gt; (H * W, C)\n\n        # reshape as x for return \n        attention_x = attention_x.permute(0, 2, 1) # (B, H * W, C) -&gt; (B, C, H * W)\n        attention_x = attention_x.reshape(B, C, H, W) # (B, C, H * W) -&gt; (B, C, H, W)\n        \n        # Convolution layer\n        attention_x = self.proj(attention_x)\n\n        # residual connection\n        x_output = attention_x + x_add\n\n        return x_output\n\n!! 할일 !!\n1) 수식 틀린거 수정\n2) denominators, dims 표시\n3) times @ dinominators = frequencies 그림\n4) SiLU 함수 식\n5) Linear layer -&gt; SiLU -&gt; Linear layer\n! 그림 잘못그림 수정해야됨(차원 틀림)"
  }
]