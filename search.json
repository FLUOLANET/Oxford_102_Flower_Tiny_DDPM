[
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html",
    "href": "Oxford_102_Flower_Tiny_DDPM.html",
    "title": "Load Dataset and Preprocess",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision import datasets\nfrom torchvision.transforms import v2\nfrom torch.utils.data import ConcatDataset, DataLoader\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#diffusion-schedule-forward-process",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#diffusion-schedule-forward-process",
    "title": "Load Dataset and Preprocess",
    "section": "Diffusion Schedule (Forward Process)",
    "text": "Diffusion Schedule (Forward Process)\n일단 Linear만 하고 나중에 Cosine이랑 Offset Cosine 추가\nsig_rates^2 + nos_rates^2 = 1\n\ndef linear_diffusion_schedule(T=1000):\n  # beta rates\n  min_beta = 0.0001\n  max_beta = 0.02\n\n  # betas, alphas, alpha_bars\n  betas = torch.linspace(min_beta, max_beta, T)\n  alphas = 1 - betas\n  alpha_bars = torch.cumprod(alphas, dim=0)\n\n  # signal rates, noise rates\n  signal_rates = torch.sqrt(alpha_bars)\n  noise_rates = torch.sqrt(1 - alpha_bars)\n\n  return signal_rates, noise_rates # shape: torch.Size([1000])\n\n\n# visualize signal power & noise power\ntimesteps = torch.linspace(0, 1, 1000)\nsig_rates, nos_rates = linear_diffusion_schedule()\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n## plot signal power\naxes[0].plot(timesteps, sig_rates**2, color=\"tab:blue\", label=r\"$\\bar{\\alpha_t}$\")\naxes[0].legend()\naxes[0].set_title(\"Signal Power\")\naxes[0].grid(True, alpha=0.2)\n\n## plot noise power\naxes[1].plot(timesteps, nos_rates**2, color=\"tab:red\", label=r\"$1-\\bar{\\alpha_t}$\")\naxes[1].legend()\naxes[1].set_title(\"Noise Power\")\naxes[1].grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#model",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#model",
    "title": "Load Dataset and Preprocess",
    "section": "Model",
    "text": "Model\n\nSinusoidal Time Embedding\n\n\n\nSinusoidal_Time_Embedding.jpg\n\n\n!! 할일 !!\n1) 수식 틀린거 수정\n2) denominators, dims 표시\n3) times @ dinominators = frequencies 그림\n4) SiLU 함수 식\n5) Linear layer -&gt; SiLU -&gt; Linear layer\n\nclass TimeEmbeddingBlock(nn.Module):\n  def __init__(self, first_dim=128, global_dim=512):\n    super().__init__()\n    '''self.dims = torch.arange(first_dim // 2) -&gt; saved on CPU!''' #dims: [0, 1, 2, 3, ..., {(first_dim//2 - 1)=63}]\n    self.register_buffer('dims', torch.arange(first_dim // 2)) # auto move to GPU\n\n    # set mlp phase(128 -&gt; Linear Layer -&gt; 512 -&gt; SiLU -&gt; 512 -&gt; Linear Layer -&gt; 512)\n    self.mlp = nn.Sequential(\n        nn.Linear(first_dim, global_dim),\n        nn.SiLU(),\n        nn.Linear(global_dim, global_dim)\n    )\n\n  # First time embeddig (BATCH_SIZE, first_dim=128)\n  def generate_first_time_embeddings(self, times):\n    B = times.shape[0] # B = BATCH_SIZE\n\n    # denominators & frequencies\n    denominators = 1/(10000**(self.dims/self.dims[-1])) # exponent covers whole range [0, 1]\n    frequencies = times.reshape(B, -1)@denominators.reshape(1, -1) # (BATCH_SIZE, (first_dim//2) = 64)\n\n    # make each half(cos, sin) and concatenate\n    odd_half = torch.cos(frequencies) # left half\n    even_half = torch.sin(frequencies) # right half\n    first_time_embeddings = torch.cat((odd_half, even_half), dim=1) #(BATCH_SIZE, first_dim=128)\n\n    return first_time_embeddings # returns (B, fist_dim=128)\n\n  # Global time embeddings (BATCH_SIZE, global_dim=512)\n  def forward(self, times):\n    x = self.generate_first_time_embeddings(times) # generates first time embeddings (BATCH_SIZE, first_dim=128)\n    global_time_embeddings = self.mlp(x) # generates global time embeddings (BATCH_size, global_dim=512)\n\n    return global_time_embeddings # returns (B, global_dim=512). It must be reshaped to (B, C)\n\n\n\nResidual Block\n\n\n\nResBlock.jpg\n\n\n\nclass ResBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, dim_t=512): # fixed num_groups 32\n    super().__init__()\n\n    self.proj1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n    self.proj2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)\n\n    # If in/out channels are different -&gt; we need 1*1 Conv / else -&gt; Identitiy\n    if in_channels != out_channels:\n      self.proj_add = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1)\n    else:\n      self.proj_add = nn.Identity()\n\n    self.proj_t = nn.Sequential(\n        nn.SiLU(),                      # i) activation(SiLU)\n        nn.Linear(dim_t, out_channels), # ii) reshape by a Linear Layer\n    )\n\n    self.groupnorm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n    self.groupnorm2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n\n    self.dropout = nn.Dropout(p=0.1)\n\n  def forward(self, x, global_time_emb):\n    # prepare for residual connection\n    x_add = x # save x\n    x_add = self.proj_add(x_add) # reshape if needed\n\n    # Before Adding Time Embedding\n    x = self.groupnorm1(x) # Group Norm 1\n    x = F.silu(x) # Swish\n    x = self.proj1(x) # 3*3 Conv 1\n\n    # Reshape and Add Time Embedding\n    t = self.proj_t(global_time_emb) # (BATCH, dim_t=512) -&gt; (BATCH, out_channels)\n    t = t[:, :, None, None] # (BATCH, out_channels) -&gt; (BATCH, out_channels, 1, 1)\n    x = x + t\n\n    # After Adding Time Embedding\n    x = self.groupnorm2(x) # Group Norm 2\n    x = F.silu(x) # Swish\n    x = self.dropout(x) # Droupout\n    x = self.proj2(x) # 3*3 Conv 2\n    x = x + x_add # residual connection\n\n    return x\n\n\n\nDownSampling & UpSampling\n\n\n\nDownSample.jpg\n\n\n! 그림 잘못그림 수정해야됨(차원 틀림)"
  }
]