[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Load Dataset",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid, save_image\nfrom torch.utils.data import DataLoader, ConcatDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport os\n# 1. 전처리 미리 정의\nIMG_RES = (64, 64)\nBATCH_SIZE = 64\n\ntransform = transforms.Compose([\n    transforms.Resize(IMG_RES),\n    transforms.ToTensor(), # 1) [0.0, 1.0] 실수 범위로 압축, 2) (H, W, C) -&gt; (C, H, W)\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 각각의 RGB 채널 평균 0.5, 표준편차 0.5로 Norm\n])\n\n# 2. Load train/validation/test datasets\ntrain_set = torchvision.datasets.Flowers102(root='./data', split='train', download=True, transform=transform)\nval_set = torchvision.datasets.Flowers102(root='./data', split='val', download=True, transform=transform)\ntest_set = torchvision.datasets.Flowers102(root='./data', split='test', download=True, transform=transform)\n\n# 3. 데이터셋 3종류 하나로 합치기(validation, test 불필요)\ndataset = ConcatDataset([train_set, val_set, test_set])\n\n# 4. 필요없는 변수 삭제 -&gt; 메모리 절약\ndel train_set, val_set, test_set\n\n# 5. dataloader\ndataloader = DataLoader(\n    dataset,\n    batch_size=BATCH_SIZE, # 64개씩 batch 만들기\n    shuffle=True, # 섞기\n    num_workers=0,\n    drop_last=True, # 자투리 버리기\n    pin_memory=True,\n)\n\n100%|██████████| 345M/345M [00:09&lt;00:00, 37.0MB/s]\n100%|██████████| 502/502 [00:00&lt;00:00, 2.14MB/s]\n100%|██████████| 15.0k/15.0k [00:00&lt;00:00, 40.6MB/s]\n# shape 확인\niterForShapeCheck = iter(dataloader)\noneBatchTensorForShapeCheck = next(iterForShapeCheck)[0]\nprint(f\"Shape of One Batch: {oneBatchTensorForShapeCheck.shape}\")\n\nShape of One Batch: torch.Size([64, 3, 64, 64])\n# 10개만 시각화해서 확인\n## 1) batch 하나 준비\ndataIterator = iter(dataloader) # dataloader은 iterable 하지만 iterator는 아님 -&gt; iterator로 만들어주기\nimages, labels = next(dataIterator) # images, labels에 batch 하나가 담김\n\n## 2) 도화지 정의\nfig, axes = plt.subplots(1, 10, figsize=(15, 3)) # fig: 전체 도화지, axes: 각각의 칸(numpy array)\nplt.subplots_adjust(wspace=0.1) # 이미지 사이 간격\n\n## 3) 그림 그리기\nfor i in range(10):\n  img = images[i]\n  img = (img * 0.5) + 0.5 # Normalize 했던거 원래대로 복구\n  img = img.permute(1, 2, 0) # (C, H, W) -&gt; (H, W, C)\n  axes[i].imshow(img.cpu().numpy()) # numpy(): Tensor -&gt; Numpy Array\n  axes[i].axis('off') # 눈금제거\n\nplt.suptitle(\"10 Images from Oxford 102 Flowers\", y=0.8)\nplt.show()"
  },
  {
    "objectID": "index.html#diffusion-schedule-forward-process",
    "href": "index.html#diffusion-schedule-forward-process",
    "title": "Load Dataset",
    "section": "Diffusion Schedule (Forward Process)",
    "text": "Diffusion Schedule (Forward Process)\n\n# 3가지 diffusion schedules - Linear/Cosine/Offset Cosine\nclass DiffusionSchedules:\n  def __init__(self, T=1000):\n    self.T = T\n    self.times = torch.linspace(0, 1, T) # [0.000, 0.001, 0.002, ... 0.999, 1.000]\n\n  ## 1) Linear Diffusion Schedules\n  ### t 지나면서 beta가 linear 하게 커짐\n  def linear_diffusion_schedule(self):\n    ## beta 범위 설정\n    min_beta = 0.0001\n    max_beta = 0.02\n\n    # betas, alphas, alpha_bars\n    betas = min_beta + self.times * (max_beta - min_beta)\n    alphas = 1 - betas\n    alpha_bars = torch.cumprod(alphas, dim=0) # coumprod: 누적 곱셈 [2, 3, 5] -&gt; [2, 6, 30]\n\n    # noise, blur rates(tensor)\n    noise_rates = torch.sqrt(1 - alpha_bars)\n    blur_rates = torch.sqrt(alpha_bars)\n\n    return blur_rates, noise_rates # 앞에 곱하는걸 앞에 썼음.\n\n\n  ## 2) Cosine Diffusion Schedules\n  ### sin^2 + cos^2 = 1 이용한 schedule\n  def cosine_diffusion_schedule(self):\n    blur_rates = torch.cos(self.times * math.pi / 2)\n    noise_rates = torch.sin(self.times * math.pi / 2)\n\n    return blur_rates, noise_rates\n\n\n  ## 3) Offset Cosine Schedules\n  ### 너무 어둡거나 밝은 이미지 생성 방지\n  def offset_diffusion_schedule(self):\n    min_blur_rate = 1e-3\n    max_blur_rate = 0.999\n\n    max_blur = torch.tensor(max_blur_rate).clamp(-1+1e-6, 1-1e-6)\n    min_blur = torch.tensor(min_blur_rate).clamp(-1+1e-6, 1-1e-6)\n\n    # min, max의 cos 각도\n    start_angle = torch.acos(max_blur)\n    end_angle   = torch.acos(min_blur)\n\n    # 각도를 linear하게 세팅해서 sin, cos 사용\n    diffusion_angles = start_angle + self.times * (end_angle - start_angle)\n    blur_rates = torch.cos(diffusion_angles)\n    noise_rates = torch.sin(diffusion_angles)\n\n    return blur_rates, noise_rates\n\n\nSchedules Visualizations\n\n# Schedule별 blur_rates, noise_rates 시각화\nschedules = DiffusionSchedules(T=1000) # Schdule들 모아놓은 class 호출\nlinear_blurs, linear_noises = schedules.linear_diffusion_schedule()\ncosine_blurs, cosine_noises = schedules.cosine_diffusion_schedule()\noffset_blurs, offset_noises = schedules.offset_diffusion_schedule()\ndiffusion_times = schedules.times\n\n\n#1. blurs: alpha_t bar\nplt.plot(diffusion_times, linear_blurs**2, color=\"#990099\", label=\"linear Schedule\")\nplt.plot(diffusion_times, cosine_blurs**2, color=\"#3b3eac\", label=\"cosine Schedule\")\nplt.plot(diffusion_times, offset_blurs**2, color=\"#329262\", label=\"Offset Cosine Schedule\")\n\nplt.xlabel(\"t / T\")\nplt.ylabel(r\"$\\bar{\\alpha_t}$ (Blur)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n#2. noises: 1 - alpha_t bar\nplt.plot(diffusion_times, linear_noises**2, label=\"linear Schedule\")\nplt.plot(diffusion_times, cosine_noises**2, label=\"cosine Schedule\")\nplt.plot(diffusion_times, offset_noises**2, label=\"Offset Cosine Schedule\")\n\nplt.xlabel(\"t / T\")\nplt.ylabel(r\"$1-\\bar{\\alpha_t}$ (Noise)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "index.html#build-parts-for-model",
    "href": "index.html#build-parts-for-model",
    "title": "Load Dataset",
    "section": "Build Parts for Model",
    "text": "Build Parts for Model\n\nSinusoidal Embedding\n\nclass SinusoidalTimeEmbedding(nn.Module):\n  # class가 호출되면 -&gt; __init__실행 -&gt; nn.Module(pytorch)의 __init__ 실행\n  def __init__(self, dim):\n    super().__init__()  # super(): 상속받은 부모 클래스인 nn.Module\n    assert dim % 2 == 0 and dim &gt;= 4, \"time embedding dim must be even and &gt;= 4\"\n    self.dim = dim  # self: SinusoidalTimeEmbedding로 만든 instance\n\n  def forward(self, time):\n    # input: time - (Batch_Size, ) shape의 1D tensor\n    # output: embeddings - (Batch_Size, dim) shpae의 2D tensor\n\n    device = time.device # time이 있을 공간(CPU or GPU)이 나중에 연산을 할 기준 공간\n\n    # frequency: 1 / 10000^(2i / dim) -&gt; 1 / 10000^(i / half_dim - 1) [0부터 시작하니까]\n    # log 씌워서 계산 (컴퓨터가 계산하기 더 좋다고 함)\n    # - {i / (half_dim - 1)} * log(10000) = -i * {log(10000) / half_dim - 1}\n\n    ## 1. half_dim 계산\n    half_dim = self.dim // 2\n\n    ## 2. log(10000) / half_dim - 1\n    pre_frequency = math.log(10000) / (half_dim - 1)\n\n    ## 3. i 만들기\n    i = torch.arange(half_dim, device=device)\n\n    ##4. frequency 완성\n    frequency = torch.exp(-i * pre_frequency)\n\n    # time(Batch_size, )와 frequency(half_dim, ) 텐서 변형해서 행렬곱 -&gt; half_embeddings(Batch_size, half_dim)\n    half_embeddings = time[:, None] * frequency[None, :]\n\n    # half_embeddings에 sin, cos 적용해서 수평으로 붙이기 -&gt; embeddings(Batch_size, dim) 완성\n    embeddings = torch.cat((half_embeddings.sin(), half_embeddings.cos()), dim=-1)\n\n    return embeddings\n\n\nVisualization\n\n# instance\nembedding_dim = 32\ntime_embedder = SinusoidalTimeEmbedding(embedding_dim) # sinusoidal 인스턴스 생성(32차원으로)\ntimes_sample = torch.arange(1000) # t = 0 ~ 999\nembeddings = time_embedder(times_sample) # time_embedder.forward() 안 해도 forward 호출해준다 (pytorch에 call함수로 잘 설계돼있다)\n\n# plot\nplt.figure(figsize=(12, 5))\nembedding_matrix = embeddings.cpu().numpy().T # embedding을 transpose -&gt; x축: t, y축: dim\n# aspect='auto': 픽셀 하나 정사각형 고집 부리지말고 figsize에 맞춰서 꽉 채워라\n# origin='lower': 수학 그래프 그리듯이 원점을 가장 아래에 둬라\nplt.imshow(embedding_matrix, aspect='auto', cmap='coolwarm', origin='lower')\nplt.title(\"Sinusoidal Time Embeddings with 1000 Timesteps\")\nplt.xlabel(\"Time Step (t)\")\nplt.ylabel(\"Dimension\")\nplt.colorbar(label=\"Value\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nResidual Block\n\nNormalization 정리\n(N=64, C=64, H=64, W=64) 기준 - Batch Norm: Batch 1 안에서 Channel 1을 끌어모아서 모든 픽셀에 대해 평균, 표준편차 구함 -&gt; Batch 1 안에서 C(채널 개수)개의 평균과 표준편차 나옴 -&gt; N 고정, C고정 - Layer Norm: Img 1에서 모든 픽셀에 대해 평균, 표준편차 구함 -&gt; Batch 하나 당 Batch_Size개의 평균과 표준편차 나옴 -&gt; C, H, W 고정 - Group Norm: Img 1, Channel 1, 2 에서 모든 픽셀에 대해 평균, 표준편차 구함 -&gt; 보통 사진 하나 당 그룹을 32개로 고정 -&gt; Batch 하나 당 32 * Batch_Size 개의 평균과 표준편차 나옴 -&gt; N, H, W 고정, C 그룹화 고정\n\nclass ResidualBlock(nn.Module):\n  def __init__(self, input_channels, output_channels, time_emb_dim, groups=8): # groups: GroupNorm 할 때 그룹개수(32개가 국룰이지만 채널 개수 충분하지 않아서 8로)\n    super().__init__()\n\n    # part 1\n    ## 3x3 convolution layer에 통과시킨다. padding 1로 줘야 projection 의 사이즈가 원래 이미지 사이즈랑 같아짐\n    ## poj_size = img_size - conv_size + 1 에서 conv_size = 3 이므로 poj_size = img_size - 2 -&gt; img_size에 2 더해줘야 같아짐\n    self.proj1 = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n    ## GroupNorm\n    self.norm1 = nn.GroupNorm(groups, output_channels)\n    ## Activation function: SiLU = x * sigma(x), where sigma(x) = sigmoid\n    self.act1 = nn.SiLU()\n\n    # time embedding part\n    self.time_embedding = nn.Sequential(\n        nn.SiLU(), # time embedding을 SiLU에 통과시켜서 비선형성 부여\n        nn.Linear(time_emb_dim, output_channels) # FFN 통과시켜서 output_channel이랑 time_emb_dim 똑같이 만들어줌\n    )\n\n    # part 2\n    self.proj2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n    self.norm2 = nn.GroupNorm(groups, output_channels)\n    self.act2 = nn.SiLU()\n\n    # residual connection part\n    ## channel 다르면 1x1 convolution으로 맞춰준다\n    if input_channels != output_channels:\n      self.connection = nn.Conv2d(input_channels, output_channels, kernel_size=1) # (input_channels, 1, 1)인 kernel을 output_channels개 준비해서 convolution\n    else:\n      self.connection = nn.Identity() # 그냥 통과(y = x)\n\n  def forward(self, x, t):\n    # x(img): (Batch_Size, input_channels, H, W)\n    # t: (Batch_Size, time_emb_dim)\n    h_for_connection = x # Residual Connection 위해서 저장\n\n    # part 1\n    h = self.proj1(x)\n    h = self.norm1(h)\n    h = self.act1(h)\n\n    # time embedding adding part\n    time_emb = self.time_embedding(t) # time embedding 변환 -&gt; (Batch_Size, output_channels)\n    h = h + time_emb[:, :, None, None] # time_emb: (Batch_Size, output_channels) -&gt; (Batch_Size, output_channels, 1, 1) (H, W)가 (1, 1)이 아니어도 같은 값 다 더해짐\n\n    # part 2\n    h = self.proj2(h)\n    h = self.norm2(h)\n    h = self.act2(h)\n\n    # residual connection part(return)\n    return h + self.connection(h_for_connection)\n\n\n\n\nAttention Block\n\nclass AttentionBlock(nn.Module):\n  def __init__(self, channels, groups=8):\n    super().__init__()\n    #1. Group Norm\n    self.norm = nn.GroupNorm(groups, channels)\n    #2. Q, K, V 만들 1x1 Convolution Layer\n    self.qkv_layer = nn.Conv2d(channels, channels * 3, kernel_size=1)\n    #3. 마무리 projection할 Convolution Layer\n    self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n\n  def forward(self, x):\n    B, C, H, W = x.shape # shape 정보 필요\n    h = x # for skip-connection\n    # Norm, QKV\n    qkv = self.qkv_layer(self.norm(h))\n    # (B, 3C, H, W) -&gt; (B, 3C, H*W) -&gt; (B, H*W, 3C)\n    qkv = qkv.reshape(B, C * 3, -1).permute(0, 2, 1)\n    # Query, Key, Value 분해\n    q, k, v = qkv.chunk(3, dim=-1)\n\n    # Attention score\n    ## softmax(Q * K^T/sqrt(d))\n    scale = int(C) ** (-0.5)\n    attn = torch.bmm(q, k.transpose(1, 2)) * scale # bmm = batch matrix multiplication\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    ## softmax(Q * K^T/sqrt(d)) * v\n    h = torch.bmm(attn, v)\n\n    # 원래 shape으로 복구\n    # (B, H*W, C) -&gt; (B, C, H*W) -&gt; (B, C, H, W)\n    h = h.permute(0, 2, 1).reshape(B, C, H, W)\n\n    # final projection, add(skip-connection)\n    h = self.proj(h)\n    return x + h\n\n\n\nDown Block\n\nclass DownBlock(nn.Module):\n  def __init__(self, input_channels, output_channels, time_emb_dim, groups=8):\n    super().__init__()\n\n    # part 1: residual block 2개 쌓기\n    self.res_block1 = ResidualBlock(input_channels, output_channels, time_emb_dim, groups)\n    self.res_block2 = ResidualBlock(output_channels, output_channels, time_emb_dim, groups)\n\n    # part 2: Downsampling\n    self.downsample = nn.Conv2d(output_channels, output_channels, kernel_size=3, stride=2, padding=1) # 두 칸씩 이동해서 픽셀 수 절반으로 줄임\n\n  def forward(self, x, t):\n    # x: B(Batch_size, input_channels, H, W)\n\n    # residual block 두 번 통과\n    x = self.res_block1(x, t)\n    x = self.res_block2(x, t)\n\n    # downsampling\n    h = x\n    x = self.downsample(x) # (Batch_Size, output_channels, H/2, W/2)\n    return x, h\n\n\n\nUp Block\n\nclass UpBlock(nn.Module):\n  def __init__(self, input_channels, output_channels, time_emb_dim, groups=8):\n    super().__init__()\n\n    # part1: upsampling\n    ## Transposed Convolution: 픽셀 한 칸을 여러 칸으로 흩뿌리기(Convoludtion의 역연산) -&gt; 2배로 커짐\n    self.upsample = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2) # 2*2 영역에 뿌리기, 새 도화지에서 2칸씩 이동\n\n    # part2: residual block 2개 쌓기\n    self.res_block1 = ResidualBlock(output_channels * 2, output_channels, time_emb_dim, groups) # Residual connection 한 것과 올라온 x 합칠 것\n    self.res_block2 = ResidualBlock(output_channels, output_channels, time_emb_dim, groups)\n\n  def forward(self, x, residual, t):\n    # x: (Batch_Size, input_channels, H, W) -&gt; down block 거쳐온 이미지\n    # residual: (Batch_Size, output_channels, H*2, W*2) -&gt; Down Block에서 저장해둔 것\n\n    # Upsampling(H, W -&gt; 2H, 2W)\n    x = self.upsample(x)\n\n    # Concatenation\n    ## dim=1 방향: Channel 방향\n    ## (B, C, H, W) -&gt; (B, 2C, H, W)\n    x = torch.cat([x, residual], dim=1)\n\n    # ResBlock 통과시키기\n    x = self.res_block1(x, t)\n    x = self.res_block2(x, t)\n\n    return x\n\n\n\nU-Net\n\nclass UNet(nn.Module):\n  def __init__(self, channel_in = 3, channel_out = 3, time_dim = 32):\n    super().__init__()\n\n    #1. Time Embedding\n    self.time_dim = time_dim\n    self.time_mlp = nn.Sequential(\n        SinusoidalTimeEmbedding(time_dim), # time embedding 생성\n        nn.Linear(time_dim, time_dim), # FFN\n        nn.SiLU() # activation function -&gt; 선형성 깨뜨리기\n    )\n\n    #2. Initial Image Transform(3 Channels -&gt; 64 Channels)\n    self.conv_init = nn.Conv2d(channel_in, 64, kernel_size=3, padding=1)\n\n    #3. Down\n    self.down1 = DownBlock(64, 128, time_dim) # 64 -&gt; 128\n    self.down2 = DownBlock(128, 256, time_dim) # 128 -&gt; 256\n\n    #4. Bottom\n    self.bottom1 = ResidualBlock(256, 256, time_dim)\n    self.attn_bottom = AttentionBlock(256) # Attention Block\n    self.bottom2 = ResidualBlock(256, 256, time_dim)\n\n    #5. Up\n    self.up1 = UpBlock(256, 256, time_dim)\n    self.up2 = UpBlock(256, 128, time_dim)\n\n    #6. Output\n    self.conv_out = nn.Conv2d(128, channel_out, kernel_size=1)\n\n  def forward(self, x, t):\n    t = self.time_mlp(t) # time embedding\n    x = self.conv_init(x) # initial transform\n    x, h1 = self.down1(x, t) # down, save input\n    x, h2 = self.down2(x, t)\n    x = self.bottom1(x, t) # bottom\n    x = self.attn_bottom(x)\n    x = self.bottom2(x, t)\n    x = self.up1(x, h2, t) # up(skip connection)\n    x = self.up2(x, h1, t)\n\n    return self.conv_out(x)"
  },
  {
    "objectID": "index.html#build-diffusion-model",
    "href": "index.html#build-diffusion-model",
    "title": "Load Dataset",
    "section": "Build Diffusion Model",
    "text": "Build Diffusion Model\n\nclass DiffusionModel(nn.Module):\n  def __init__(self, network):\n    super().__init__()\n    self.network = network # U-Net\n    self.T = 1000 # number of timestep\n    schedules = DiffusionSchedules(T=self.T) # Scheduler instance\n    self.blur_rates, self.noise_rates = schedules.linear_diffusion_schedule() # scheduler: linear로 선택\n\n    # Buffer로 등록(GPU로 이동)\n    self.register_buffer(\"blur_rates_buffer\", self.blur_rates)\n    self.register_buffer(\"noise_rates_buffer\", self.noise_rates)\n\n  def forward(self, x_0):\n    # x_0: 원본 이미지\n    batch_size = x_0.shape[0]\n    device = x_0.device # 마찬가지로 기준 연산공간 지정\n\n    #1. random t sampling: (batch_size, )만큼\n    t = torch.randint(0, self.T, (batch_size, ), device=device)\n\n    #2. 뽑은 t에 대해 blur_rate, noise_rate 준비 -&gt; (batch_size, 1, 1, 1) shape으로\n    blur_rate = self.blur_rates_buffer[t].reshape(-1, 1, 1, 1)\n    noise_rate = self.noise_rates_buffer[t].reshape(-1, 1, 1, 1)\n\n    #3. random noise -&gt; label\n    noise = torch.randn_like(x_0) # x_0랑 같은 모양으로 N(0, 1)에서 sampling\n\n    #4. Forward Process\n    img_noisy = blur_rate * x_0 + noise_rate * noise\n\n    #5. 망가진 이미지, t 주고 U-Net이 Noise 예측\n    pred_noise = self.network(img_noisy, t)\n\n    #6. Loss\n    loss = nn.functional.mse_loss(pred_noise, noise)\n\n    return loss\n\n  @torch.no_grad()\n  def sample(self, image_size, batch_size=8, channels=3):\n    device = next(self.parameters()).device\n\n    # 1. 완전한 noise에서 시작\n    img = torch.randn((batch_size, channels, image_size, image_size), device=device)\n\n    epsilon = 1e-5\n\n    # 2. T-1 -&gt; 0으로 denoising\n    for t in range(self.T - 1, -1, -1):\n        t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n\n        # Noise prediction\n        pred_noise = self.network(img, t_batch)\n\n        # Current timestep의 blur/noise rates\n        current_blur = self.blur_rates_buffer[t]\n        current_noise = self.noise_rates_buffer[t]\n\n        # Previous timestep의 blur rate\n        if t &gt; 0:\n            previous_blur = self.blur_rates_buffer[t - 1]\n        else:\n            previous_blur = torch.tensor(1.0, device=device)\n\n        # Alpha_bar 계산\n        alpha_bar_t = current_blur ** 2\n        alpha_bar_prev = previous_blur ** 2\n\n        # Alpha_t 계산\n        if t &gt; 0:\n            alpha_t = alpha_bar_t / torch.clamp(alpha_bar_prev, min=epsilon)\n            alpha_t = alpha_t.clamp(epsilon, 1.0)\n        else:\n            alpha_t = torch.tensor(1.0, device=device)\n\n        # Beta_t 계산\n        beta_t = (1.0 - alpha_t).clamp(min=0.0, max=1.0)\n\n        # Mean 계산\n        sqrt_one_minus_alpha_bar_t = torch.sqrt(torch.clamp(1.0 - alpha_bar_t, min=epsilon))\n        coef = beta_t / sqrt_one_minus_alpha_bar_t\n\n        sqrt_alpha_t = torch.sqrt(torch.clamp(alpha_t, min=epsilon))\n        mean = (1.0 / sqrt_alpha_t) * (img - coef * pred_noise)\n\n        # Variance와 noise 추가\n        if t &gt; 0:\n            numerator = beta_t * (1.0 - alpha_bar_prev)\n            denominator = torch.clamp(1.0 - alpha_bar_t, min=epsilon)\n            posterior_var = numerator / denominator\n            posterior_var = posterior_var.clamp(min=0.0)\n\n            sigma_t = torch.sqrt(posterior_var)\n            z = torch.randn_like(img)\n            img = mean + sigma_t * z\n        else:\n            img = mean\n\n    img = img.clamp(-1.0, 1.0)\n    img = (img + 1.0) / 2.0\n\n    return img"
  },
  {
    "objectID": "index.html#train",
    "href": "index.html#train",
    "title": "Load Dataset",
    "section": "Train",
    "text": "Train\n\n#1. Model, Optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nunet = UNet(channel_in=3, channel_out=3, time_dim=32).to(device)\nmodel = DiffusionModel(network=unet).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n#2. Folders for saving\nOUT_DIR = \"./output\"\nCKPT_DIR = \"./checkpoint\"\nos.makedirs(OUT_DIR, exist_ok=True)\nos.makedirs(CKPT_DIR, exist_ok=True)\n\n#3. Learning Loop\nEPOCHS = 25\nNUM_SAMPLE = 10\nIMG_SIZE = 64\nSAMPLE_EVERY = 1\nSAVE_CKPT_EVERY = 50  # 50 epoch마다 체크포인트 저장\n\n# Loss 기록용\nloss_history = []\n\nprint(f\"Starting training for {EPOCHS} epochs...\")\nprint(f\"Device: {device}\")\nprint(f\"Total batches per epoch: {len(dataloader)}\")\n\nfor epoch in range(EPOCHS):\n    model.train()\n    epoch_loss = 0.0\n    num_batches = 0\n\n    for images, _ in dataloader:\n        images = images.to(device)\n\n        optimizer.zero_grad()\n        loss = model(images)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        num_batches += 1\n\n    avg_loss = epoch_loss / max(1, num_batches)\n    loss_history.append(avg_loss)\n\n    # Progress print\n    print(f\"[Epoch {epoch+1:3d}/{EPOCHS}] avg_loss = {avg_loss:.6f}\")\n\n    # 샘플 이미지 생성 (SAMPLE_EVERY마다)\n    if (epoch + 1) % SAMPLE_EVERY == 0 or epoch == 0:\n        model.eval()\n        with torch.no_grad():\n            torch.manual_seed(42)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(42)\n\n            samples = model.sample(image_size=IMG_SIZE, batch_size=NUM_SAMPLE, channels=3)\n            grid = make_grid(samples.cpu(), nrow=10, padding=2, pad_value=1.0)\n            grid = grid.clamp(0, 1)\n\n            save_path = f\"{OUT_DIR}/generated_img_{epoch+1:03d}.png\"\n            save_image(grid, save_path)\n\n            plt.figure(figsize=(10, 2))\n            plt.imshow(grid.permute(1, 2, 0).numpy())\n            plt.axis(\"off\")\n            plt.title(f\"Generated Images at Epoch {epoch+1}\")\n            plt.tight_layout()\n            plt.show()\n            print(f\"✓ Saved image: {save_path}\")\n\n    # 체크포인트 저장 (SAVE_CKPT_EVERY마다)\n    if (epoch + 1) % SAVE_CKPT_EVERY == 0:\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_loss,\n            'loss_history': loss_history,\n        }\n        ckpt_path = f\"{CKPT_DIR}/checkpoint_epoch_{epoch+1:03d}.pt\"\n        torch.save(checkpoint, ckpt_path)\n        print(f\"Saved checkpoint: {ckpt_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training completed!\")\nprint(\"=\"*60)\n\n# 최종 Loss 그래프\nplt.figure(figsize=(12, 5))\nplt.plot(loss_history, linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Average Loss', fontsize=12)\nplt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(f\"{OUT_DIR}/loss_curve.png\", dpi=150)\nplt.show()\nprint(f\"Saved loss curve: {OUT_DIR}/loss_curve.png\")\n\n# 최종 샘플 생성\nprint(\"\\nGenerating final samples...\")\nmodel.eval()\nwith torch.no_grad():\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n\n    samples = model.sample(image_size=IMG_SIZE, batch_size=25, channels=3)\n    grid = make_grid(samples.cpu(), nrow=10, padding=2, pad_value=1.0)\n    grid = grid.clamp(0, 1)\n\n    save_path = f\"{OUT_DIR}/final_samples.png\"\n    save_image(grid, save_path)\n\n    plt.figure(figsize=(10, 2))\n    plt.imshow(grid.permute(1, 2, 0).numpy())\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n    print(f\"Saved final samples: {save_path}\")\n\nStarting training for 25 epochs...\nDevice: cuda\nTotal batches per epoch: 127\n[Epoch   1/25] avg_loss = 0.171135\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_001.png\n[Epoch   2/25] avg_loss = 0.069920\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_002.png\n[Epoch   3/25] avg_loss = 0.055337\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_003.png\n[Epoch   4/25] avg_loss = 0.049606\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_004.png\n[Epoch   5/25] avg_loss = 0.046708\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_005.png\n[Epoch   6/25] avg_loss = 0.044768\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_006.png\n[Epoch   7/25] avg_loss = 0.043179\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_007.png\n[Epoch   8/25] avg_loss = 0.041937\n\n\n\n\n\n\n\n\n\n✓ Saved image: ./output/generated_img_008.png\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/tmp/ipython-input-3804794597.py in &lt;cell line: 0&gt;()\n     30     num_batches = 0\n     31 \n---&gt; 32     for images, _ in dataloader:\n     33         images = images.to(device)\n     34 \n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py in __next__(self)\n    730                 # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    731                 self._reset()  # type: ignore[call-arg]\n--&gt; 732             data = self._next_data()\n    733             self._num_yielded += 1\n    734             if (\n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py in _next_data(self)\n    786     def _next_data(self):\n    787         index = self._next_index()  # may raise StopIteration\n--&gt; 788         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    789         if self._pin_memory:\n    790             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\n     50                 data = self.dataset.__getitems__(possibly_batched_index)\n     51             else:\n---&gt; 52                 data = [self.dataset[idx] for idx in possibly_batched_index]\n     53         else:\n     54             data = self.dataset[possibly_batched_index]\n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py in __getitem__(self, idx)\n    344         else:\n    345             sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n--&gt; 346         return self.datasets[dataset_idx][sample_idx]\n    347 \n    348     @property\n\n/usr/local/lib/python3.12/dist-packages/torchvision/datasets/flowers102.py in __getitem__(self, idx)\n     88 \n     89         if self.transform:\n---&gt; 90             image = self.transform(image)\n     91 \n     92         if self.target_transform:\n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py in __call__(self, img)\n     93     def __call__(self, img):\n     94         for t in self.transforms:\n---&gt; 95             img = t(img)\n     96         return img\n     97 \n\n/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)\n   1773             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1774         else:\n-&gt; 1775             return self._call_impl(*args, **kwargs)\n   1776 \n   1777     # torchrec tests the code consistency with the following code\n\n/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\n   1784                 or _global_backward_pre_hooks or _global_backward_hooks\n   1785                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1786             return forward_call(*args, **kwargs)\n   1787 \n   1788         result = None\n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py in forward(self, img)\n    352             PIL Image or Tensor: Rescaled image.\n    353         \"\"\"\n--&gt; 354         return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n    355 \n    356     def __repr__(self) -&gt; str:\n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py in resize(img, size, interpolation, max_size, antialias)\n    475             warnings.warn(\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\")\n    476         pil_interpolation = pil_modes_mapping[interpolation]\n--&gt; 477         return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)\n    478 \n    479     return F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_pil.py in resize(img, size, interpolation)\n    251         raise TypeError(f\"Got inappropriate size arg: {size}\")\n    252 \n--&gt; 253     return img.resize(tuple(size[::-1]), interpolation)\n    254 \n    255 \n\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py in resize(self, size, resample, box, reducing_gap)\n   2319                 )\n   2320 \n-&gt; 2321         return self._new(self.im.resize(size, resample, box))\n   2322 \n   2323     def reduce(\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]