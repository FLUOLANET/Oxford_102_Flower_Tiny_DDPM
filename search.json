[
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html",
    "href": "Oxford_102_Flower_Tiny_DDPM.html",
    "title": "(Settings)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision import datasets\nfrom torchvision.transforms import v2\nfrom torch.utils.data import ConcatDataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n# Priorities: CUDA -&gt; MPS -&gt; CPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Device: CUDA (NVIDIA GPU)\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Device: MPS (Mac Silicon)\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Device: CPU\")\n\nDevice: CUDA (NVIDIA GPU)"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#load-dataset-and-preprocess",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#load-dataset-and-preprocess",
    "title": "(Settings)",
    "section": "1. Load Dataset and Preprocess",
    "text": "1. Load Dataset and Preprocess\n\ntransform\nToTensor:\n1) 값이 존재하는 범위를 Z에서 R로\n2) 범위 줄이기 -&gt; gradient(미분계수) 폭발 방지\nNormalize: 채널 3개라서 3개씩\n! num_workers: ?? 메인 프로세스는 학습만, 별도의 프로세스가 다음 배치 미리 읽어옴\n! pin_memory: ?? 데이터를 RAM의 아무데나 넣는게 아니라 고정된 위치에 넣음\n\nIMG_SIZE = 64\nBATCH_SIZE = 64\n\n# define trasform\ntransform = v2.Compose([\n    v2.Resize(IMG_SIZE), #1. resize shorter edge into 64\n    v2.CenterCrop(IMG_SIZE), #2. crop at the center(rectangle image -&gt; square image)\n    v2.ToImage(), #3. (H, W, C) -&gt; (C, H, W)\n    v2.ToDtype(torch.float32, scale=True), #4. Z[0, 255] -&gt; R[0.0, 1.0]\n    v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #5. Normalize [0.0, 1.0] -&gt; [-1.0, 1.0]\n])\n\n# load data\ntrain_set = datasets.Flowers102(root='./data', split='train', download=True, transform=transform)\nval_set = datasets.Flowers102(root='./data', split='val', download=True, transform=transform)\ntest_set = datasets.Flowers102(root='./data', split='test', download=True, transform=transform)\n\n# concaterate into one train dataset\ndataset = ConcatDataset([train_set, val_set, test_set])\ndel train_set, val_set, test_set\n\n# build dataloader\ndataloader = DataLoader(\n    dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n    drop_last=True,\n)\n\n100%|██████████| 345M/345M [00:19&lt;00:00, 17.3MB/s] \n100%|██████████| 502/502 [00:00&lt;00:00, 1.37MB/s]\n100%|██████████| 15.0k/15.0k [00:00&lt;00:00, 33.5MB/s]\n\n\n\n# Check batch shape\ndataloader_iterator = iter(dataloader) # dataloader: iterable -&gt; dataloader_iterator: iterater\nsample_batch = next(dataloader_iterator)[0] # bring only features of a batch, not labels\nprint(f\"Shape of Each Batch: {sample_batch.shape}\")\n\nShape of Each Batch: torch.Size([64, 3, 64, 64])\n\n\n\n# Visualize 10 sample images\n## fetch a single batch for visualization\nimages = next(dataloader_iterator)[0]\n\n## initialize subplots\nfig, axes = plt.subplots(1, 10, figsize=(15, 3)) # fig: top-level container, # axes: numpy array of each grid\nplt.subplots_adjust(wspace=0.1)\n\n## plot each subplots\nfor i in range(10):\n  img = images[i]\n  img = (img * 0.5) + 0.5 # unnormalize to [0, 1] for visualization\n  img = img.permute(1, 2, 0) # (C, H, W) -&gt; (H, W, C)\n  axes[i].imshow(img.cpu().numpy()) # display image\n  axes[i].axis('off')\n\nplt.suptitle(\"10 Sample Images from Oxford 102 Flowers\", y=0.8)\nplt.show()"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#diffusion-schedule-forward-process",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#diffusion-schedule-forward-process",
    "title": "(Settings)",
    "section": "2. Diffusion Schedule (Forward Process)",
    "text": "2. Diffusion Schedule (Forward Process)\n일단 Linear만 하고 나중에 Cosine이랑 Offset Cosine 추가\nsig_rates^2 + nos_rates^2 = 1\n\ndef linear_diffusion_schedule(T=1000):\n  # beta rates\n  min_beta = 0.0001\n  max_beta = 0.02\n\n  # betas, alphas, alpha_bars\n  betas = torch.linspace(min_beta, max_beta, T, device=device)\n  alphas = 1 - betas\n  alpha_bars = torch.cumprod(alphas, dim=0)\n\n  # signal rates, noise rates\n  signal_rates = torch.sqrt(alpha_bars)\n  noise_rates = torch.sqrt(1 - alpha_bars)\n\n  return signal_rates, noise_rates # shape: torch.Size([1000])\n\n\n# visualize signal power & noise power\ntimesteps = torch.linspace(0, 1, 1000) # Visualization works in CPU\nsig_rates, nos_rates = linear_diffusion_schedule() # on MPS/GPU\nsig_rates = sig_rates.detach().cpu() # move to CPU\nnos_rates = nos_rates.detach().cpu()\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n## plot signal power\naxes[0].plot(timesteps, sig_rates**2, color=\"tab:blue\", label=r\"$\\bar{\\alpha_t}$\")\naxes[0].legend()\naxes[0].set_title(\"Signal Power\")\naxes[0].grid(True, alpha=0.2)\n\n## plot noise power\naxes[1].plot(timesteps, nos_rates**2, color=\"tab:red\", label=r\"$1-\\bar{\\alpha_t}$\")\naxes[1].legend()\naxes[1].set_title(\"Noise Power\")\naxes[1].grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#model",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#model",
    "title": "(Settings)",
    "section": "3. Model",
    "text": "3. Model\n\n2) Residual Block\n\n\n\nResBlock.jpg\n\n\n\ni) Sinusoidal Time Embedding\n\n\n\nSinusoidal_Time_Embedding.jpg\n\n\n\nclass TimeEmbeddingBlock(nn.Module):\n  def __init__(self, first_dim=128, global_dim=512):\n    super().__init__()\n    '''self.dims = torch.arange(first_dim // 2) -&gt; saved on CPU!''' #dims: [0, 1, 2, 3, ..., {(first_dim//2 - 1)=63}]\n    self.register_buffer('dims', torch.arange(first_dim // 2)) # auto move to GPU\n\n    # set mlp phase(128 -&gt; Linear Layer -&gt; 512 -&gt; SiLU -&gt; 512 -&gt; Linear Layer -&gt; 512)\n    self.mlp = nn.Sequential(\n        nn.Linear(first_dim, global_dim),\n        nn.SiLU(),\n        nn.Linear(global_dim, global_dim)\n    )\n\n  # First time embedding (BATCH_SIZE, first_dim=128)\n  def generate_first_time_embeddings(self, times): \n    times = times.float() # to operate with float\n    B = times.shape[0] # B = BATCH_SIZE\n\n    # denominators & frequencies\n    denominators = 1/(10000**(self.dims/self.dims[-1])) # exponent covers whole range [0, 1]\n    frequencies = times.reshape(B, -1)@denominators.reshape(1, -1) # (BATCH_SIZE, (first_dim//2) = 64)\n\n    # make each half(cos, sin) and concatenate\n    odd_half = torch.cos(frequencies) # left half\n    even_half = torch.sin(frequencies) # right half\n    first_time_embeddings = torch.cat((odd_half, even_half), dim=1) #(BATCH_SIZE, first_dim=128)\n\n    return first_time_embeddings # returns (B, fist_dim=128)\n\n  # Global time embeddings (BATCH_SIZE, global_dim=512)\n  def forward(self, times):\n    x = self.generate_first_time_embeddings(times) # generates first time embeddings (BATCH_SIZE, first_dim=128)\n    global_time_embeddings = self.mlp(x) # generates global time embeddings (BATCH_size, global_dim=512)\n\n    return global_time_embeddings # returns (B, global_dim=512). It must be reshaped to (B, C)\n\n\n\nii) Residual Block\n\nclass ResBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, dim_t=512): # fixed num_groups 32\n    super().__init__()\n\n    self.proj1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n    self.proj2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)\n\n    # If in/out channels are different -&gt; we need 1*1 Conv / else -&gt; Identitiy\n    if in_channels != out_channels:\n      self.proj_add = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n    else:\n      self.proj_add = nn.Identity()\n\n    self.proj_t = nn.Sequential(\n        nn.SiLU(),                      # i) activation(SiLU)\n        nn.Linear(dim_t, out_channels), # ii) reshape by a Linear Layer\n    )\n\n    self.groupnorm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n    self.groupnorm2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n\n    self.dropout = nn.Dropout(p=0.1)\n\n  def forward(self, x, global_time_emb):\n    # prepare for residual connection\n    x_add = x # save x\n    x_add = self.proj_add(x_add) # reshape if needed\n\n    # Before Adding Time Embedding\n    x = self.groupnorm1(x) # Group Norm 1\n    x = F.silu(x) # Swish\n    x = self.proj1(x) # 3*3 Conv 1\n\n    # Reshape and Add Time Embedding\n    t = self.proj_t(global_time_emb) # (BATCH, dim_t=512) -&gt; (BATCH, out_channels)\n    t = t[:, :, None, None] # (BATCH, out_channels) -&gt; (BATCH, out_channels, 1, 1)\n    x = x + t\n\n    # After Adding Time Embedding\n    x = self.groupnorm2(x) # Group Norm 2\n    x = F.silu(x) # Swish\n    x = self.dropout(x) # Droupout\n    x = self.proj2(x) # 3*3 Conv 2\n    x = x + x_add # residual connection\n\n    return x\n\n\n\n\n3) DownSample / UpSample\n\n# DownSample\nclass DownSample(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.downsample = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=1, stride=2)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        return x\n    \n# UpSample\nclass UpSample(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') # just upscale 1 pixel into 4 pixels\n        self.proj = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding = 1) # smooth out\n\n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.proj(x)\n        return x\n\n\n\n4) Attention Block\n\n# AttentionBlock\nclass AttentionBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n        # group normalization\n        self.groupnorm = nn.GroupNorm(num_groups=32, num_channels=dim)\n\n        # attention parameters (W_Q, W_K, W_V)\n        self.w_q = nn.Linear(dim, dim)  # w_q: (C, C)\n        self.w_k = nn.Linear(dim, dim)  # w_k: (C, C)\n        self.w_v = nn.Linear(dim, dim)  # w_v: (C, C)\n\n        # 1*1 convolution\n        self.proj = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=1)\n\n    def forward(self, x):\n        x_add = x # for residual connection\n        B, C, H, W = x.shape\n\n        # GroupNorm before attention\n        x = self.groupnorm(x)\n\n        # reshape for attention: (B, C, H, W) -&gt; (B, H * W, C)\n        x = x.reshape(B, C, -1) # (B, C, H, W) -&gt; (B, C, H * W)\n        x = x.permute(0, 2, 1) # (B, C, H * W) -&gt; (B, H * W, C)\n\n        # Query, Key, Value\n        q = self.w_q(x) # x @ w_q(k, v): (H * W, C) @ (C, C) -&gt; (H * W, C)\n        k = self.w_k(x)\n        v = self.w_v(x)\n\n        # Attention\n        inner_q_k = q @ k.transpose(-1, -2) # (H * W, C) @ (C, H * W) -&gt; (H * W, H * W)\n        attention_x = (F.softmax((inner_q_k/C**0.5), dim=-1)) @ v # (H * W, H * W) @ (H * W, C) -&gt; (H * W, C)\n\n        # reshape as x for return \n        attention_x = attention_x.permute(0, 2, 1) # (B, H * W, C) -&gt; (B, C, H * W)\n        attention_x = attention_x.reshape(B, C, H, W) # (B, C, H * W) -&gt; (B, C, H, W)\n        \n        # Convolution layer\n        attention_x = self.proj(attention_x)\n\n        # residual connection\n        x_output = attention_x + x_add\n\n        return x_output\n\n\n\n5) U-Net\n\n# Build U-Net\nclass U_Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Make Global Time Embedding generator instance\n        self.t_emb_gen = TimeEmbeddingBlock()\n\n        # Input Convolution\n        self.input_conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        \n        # Down 1\n        self.Down1_ResBlock_1 = ResBlock(in_channels=64, out_channels=64)\n        self.Down1_ResBlock_2 = ResBlock(in_channels=64, out_channels=64)\n        self.Down1_DownSample = DownSample(dim=64)\n\n        # Down 2\n        self.Down2_ResBlock_1 = ResBlock(in_channels=64, out_channels=128)\n        self.Down2_ResBlock_2 = ResBlock(in_channels=128, out_channels=128)\n        self.Down2_DownSample = DownSample(dim=128)\n\n        # Down 3 \n        self.Down3_ResBlock_1 = ResBlock(in_channels=128, out_channels=256)\n        self.Down3_AttentionBlock = AttentionBlock(dim=256)\n        self.Down3_ResBlock_2 = ResBlock(in_channels=256, out_channels=256)\n        self.Down3_DownSample = DownSample(dim=256)\n\n        # Down 4\n        self.Down4_ResBlock_1 = ResBlock(in_channels=256, out_channels=512)\n        self.Down4_ResBlock_2 = ResBlock(in_channels=512, out_channels=512)\n        self.Down4_DownSample = DownSample(dim=512)\n\n        # Middle\n        self.Middle_ResBlock_1 = ResBlock(in_channels=512, out_channels=512)\n        self.Middle_ResBlock_2 = ResBlock(in_channels=512, out_channels=512)\n        self.Middle_AttentionBlock = AttentionBlock(dim=512)\n\n        # Up 1\n        self.Up1_UpSample = UpSample(dim=512)\n        self.Up1_ResBlock_1 = ResBlock(in_channels=1024, out_channels=512)\n        self.Up1_ResBlock_2 = ResBlock(in_channels=512, out_channels=512)\n        \n        # Up 2\n        self.Up2_UpSample = UpSample(dim=512)\n        self.Up2_ResBlock_1 = ResBlock(in_channels=768, out_channels=256)\n        self.Up2_AttentionBlock = AttentionBlock(dim=256)\n        self.Up2_ResBlock_2 = ResBlock(in_channels=256, out_channels=256)\n         \n        # Up 3\n        self.Up3_UpSample = UpSample(dim=256)\n        self.Up3_ResBlock_1 = ResBlock(in_channels=384, out_channels=128)\n        self.Up3_ResBlock_2 = ResBlock(in_channels=128, out_channels=128)\n\n        # Up 4\n        self.Up4_UpSample = UpSample(dim=128)\n        self.Up4_ResBlock_1 = ResBlock(in_channels=192, out_channels=64)\n        self.Up4_ResBlock_2 = ResBlock(in_channels=64, out_channels=64)\n\n        # Output Sequence \n        self.Output_Seq = nn.Sequential(\n            nn.GroupNorm(num_groups=32, num_channels=64),\n            nn.SiLU(),\n            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)\n        )\n\n    def forward(self, x, t):\n        \n        # make time embedding \n        global_time_emb = self.t_emb_gen(t)\n        # input \n        x = self.input_conv(x)\n\n        # Down 1\n        x = self.Down1_ResBlock_1(x, global_time_emb)\n        x = self.Down1_ResBlock_2(x, global_time_emb)\n        skip_1 = x\n        x = self.Down1_DownSample(x)\n      \n        # Down 2\n        x = self.Down2_ResBlock_1(x, global_time_emb)\n        x = self.Down2_ResBlock_2(x, global_time_emb)\n        skip_2 = x\n        x = self.Down2_DownSample(x)\n\n        # Down 3\n        x = self.Down3_ResBlock_1(x, global_time_emb)\n        x = self.Down3_AttentionBlock(x)\n        x = self.Down3_ResBlock_2(x, global_time_emb)\n        skip_3 = x\n        x = self.Down3_DownSample(x)\n        \n        # Down 4\n        x = self.Down4_ResBlock_1(x, global_time_emb)\n        x = self.Down4_ResBlock_2(x, global_time_emb)\n        skip_4 = x\n        x = self.Down4_DownSample(x)\n\n        # Middle\n        x = self.Middle_ResBlock_1(x, global_time_emb)\n        x = self.Middle_AttentionBlock(x)\n        x = self.Middle_ResBlock_2(x, global_time_emb)\n\n        # Up 1\n        x = self.Up1_UpSample(x)\n        x = torch.cat((x, skip_4), dim=1)\n        x = self.Up1_ResBlock_1(x, global_time_emb)\n        x = self.Up1_ResBlock_2(x, global_time_emb)\n\n        # Up 2\n        x = self.Up2_UpSample(x)\n        x = torch.cat((x, skip_3), dim=1)\n        x = self.Up2_ResBlock_1(x, global_time_emb)\n        x = self.Up2_AttentionBlock(x)\n        x = self.Up2_ResBlock_2(x, global_time_emb)\n\n        # Up 3\n        x = self.Up3_UpSample(x)\n        x = torch.cat((x, skip_2), dim=1)\n        x = self.Up3_ResBlock_1(x, global_time_emb)\n        x = self.Up3_ResBlock_2(x, global_time_emb)\n\n        # Up 4\n        x = self.Up4_UpSample(x)\n        x = torch.cat((x, skip_1), dim=1)\n        x = self.Up4_ResBlock_1(x, global_time_emb)\n        x = self.Up4_ResBlock_2(x, global_time_emb)\n\n        # Output Sequence\n        x = self.Output_Seq(x)\n\n        return x"
  },
  {
    "objectID": "Oxford_102_Flower_Tiny_DDPM.html#train",
    "href": "Oxford_102_Flower_Tiny_DDPM.html#train",
    "title": "(Settings)",
    "section": "4. Train",
    "text": "4. Train\n\n1) Compute Loss\n\nclass DDPM(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.u_net = model\n\n        # generate signal_rates, noise_rates\n        signal_rates, noise_rates = linear_diffusion_schedule()\n        self.register_buffer('signal_rates', signal_rates) # attach to register buffer for migration to GPU\n        self.register_buffer('noise_rates', noise_rates)\n\n    def forward(self, x):\n\n        '''Phase 1: Make Input = Noised image(\"noised_x\")'''\n\n        ## 1) generate \"epsilon\"\n        epsilon = torch.randn_like(x) # noise ~ N(0, 1), shape: (B, C, H, W)\n\n        ## 2) sample \"t\" form [0, 1, 2, ..., 999] uniformly\n        t = torch.randint(0, 1000, (x.shape[0], ), device=x.device) # t: (B) int tensor\n\n        ## 3) take \"sig_rates\" and \"ns_rates\" for calculation\n        sig_rates = self.signal_rates[t] # sig_rates: (B) float tensor\n        sig_rates = sig_rates.reshape(x.shape[0], 1, 1, 1) # sig_rates: (B, 1, 1, 1)\n\n        ns_rates = self.noise_rates[t] # ns_rates: (B) float tensor\n        ns_rates = ns_rates.reshape(x.shape[0], 1, 1, 1) # ns_rates: (B, 1, 1, 1)\n\n        ## 4) make \"noised_x\"\n        noised_x = sig_rates * x + epsilon * ns_rates\n\n\n        '''Phase 2: Model Prediction'''\n        pred_epsilon = self.u_net(noised_x, t) # pred_noise: (B, 3, H, W) float tensor\n\n\n        '''Phase 3: Calculate Loss Function'''\n        ## Loss(MSE): [sum of {(epsilon - pred_epsilon)^2}] / (B * C * H * W)\n        ## mean of every pixel's error in a batch\n        loss = F.mse_loss(pred_epsilon, epsilon) \n\n        return loss\n\n\n\n2) Train Loop\n\n# Hyperparameters for train\nLEARNING_RATE = 1e-4\nEPOCH_NUM = 1\n\n# Optimization Setup \nddpm = DDPM(U_Net()).to(device) # model instance\noptimizer = torch.optim.Adam(ddpm.parameters(), lr=LEARNING_RATE) # setup optimizer\n\n# Loss Curve Ingredients\nloss_vis = []\n\n# Optimize\nddpm.train() # train mode\n\nfor epoch in range(EPOCH_NUM):\n    dataloader = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCH_NUM}\") # progress bar\n\n    for batch in dataloader:\n        \n        # optimize\n        x, _ = batch # batch: (x, labels), don't need (labels) in this case\n        x = x.to(device)\n        optimizer.zero_grad() # We don't accumulate gradient\n        loss = ddpm(x) # compute loss\n        loss.backward() # compute gradient\n        optimizer.step() # update parameters\n\n        # values for loss curve \n        loss_vis.append(loss.item()) # .item(): extract scalar not whole computational graph\n\n        # progress bar\n        dataloader.set_postfix(loss=f\"{loss.item():.4f}\")\n\nEpoch 1/1: 100%|██████████| 127/127 [00:45&lt;00:00,  2.78it/s, loss=0.0969]"
  }
]